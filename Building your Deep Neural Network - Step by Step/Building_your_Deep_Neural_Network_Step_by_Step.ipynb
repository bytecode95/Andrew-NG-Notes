{"cells":[{"cell_type":"markdown","metadata":{"id":"Q-RgGAPwNU4Z"},"source":["# Building your Deep Neural Network: Step by Step\n","\n","ඔබ විසින් පෙරදී layers දෙකක (hidden layer එකක් ඇති) neural network එකක් train කරන ආකාරය jupyter notebook එකක් ආධාරයෙන් උගෙන ඇත. මෙම mini project එකේදී ඔබ විසින් ඔබට අවශ්‍ය ඕනෑම layers ගණනක් ඇති neural network එකක් train කරන ආකාරය ඉගෙන ගනු ඇත.\n","\n","- මෙම notebook එකේදී ඔබ විසින් deep neural network එකක් නිර්මාණය කිරීමට අවශ්‍ය සියලුම functions implement කරනු ඇත.\n","- mini project එකෙන් පසුව ඔබට මෙම functions භාවිත කර image classification සඳහා deep neural network එකක් නිර්මාණය කරන ආකාරය දැක්වෙන jupyter notebook එකක් ලබා දෙනු ලැබේ.\n","\n","**මෙම activity එකෙන් පසුව ඔබට**\n","- ReLU වැනි non-linear units භාවිතා කර model එකක් improve කර ගන්නා ආකාරය\n","- hidden layers එකකට වඩා ඇති deep neural network එකක් නිර්මාණය කරගන්න ආකාරය\n","- භාවිතා කිරීමට පහසු neural network class එකක් නිර්මාණය කරගන්නා ආකාරය\n","ගැන අදහසක් ලබා ගැනීමට හැකි වනු ඇත.\n","\n","\n","**Notation**:\n","- Superscript $[l]$ මගින් $l$ වන layer එක සමග සම්බන්ද quantity එකක් දක්වනු ලබයි.\n","    - Example: $a^{[L]}$ යනු $L$ වන layer එකෙහි activation එක වේ. $W^{[L]}$ and $b^{[L]}$ යනු $L$ වන layer එකෙහි parameters වේ.\n","- Superscript $(i)$ මගින් $i$ වන example එක සමග සම්බන්ද quantity එකක් දක්වනු ලබයි.\n","    - Example: $x^{(i)}$ යනු $i$ වන training example එක වේ.\n","- Lowerscript $i$ මගින්, vector එකක $i$ වන entry එක දක්වනු ලබයි.\n","    - Example: $a^{[l]}_i$ යනු $l$ වන layer එකෙහි activation එකෙහි $i$ වන entry එක වේ\n","\n","\n","Let's get started!"]},{"cell_type":"code","source":["from google.colab import drive\n","import sys\n","drive.mount('/content/gdrive')\n","sys.path.append('/content/gdrive/MyDrive/ML/Building your Deep Neural Network - Step by Step')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fsPX8YPTEHEO","executionInfo":{"status":"ok","timestamp":1696479766500,"user_tz":-330,"elapsed":4187,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"}},"outputId":"6efdea3c-fe7d-4cd9-d6bd-8ac133bf64e0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"w7ak8wHbNU4h"},"source":["## 1 - Packages\n","\n","මෙම පැවරුම තුළදී ඔබට අවශ්‍ය වන සියලු packages පළමුව import කරමු.\n","- [Numpy] (www.nphay.org) යනු පයිතන් සමඟ scientific computing සඳහා භාවිතා වන ප්‍රධාන පැකේජයයි.\n","- [Matplotllib] (http://matallotlib.org) යනු පයිතන් හි graph plotting සඳහා වන library එකකි.\n","- Dnn_utils මෙම නෝට්බුක් සඳහා අවශ්ය functions කිහිපයක් සපයන, ඔබට ලබාදෙන ලද folder එක තුල ඇති තවත් python file එකකි..\n","- testcase ඔබේ කාර්යයන්හි නිවැරදිබව තක්සේරු කිරීම සඳහා tests සපයයි.\n","- np.random.seed (1) සියලු random function calls වෙනස් නොවී තබා ගැනීමට භාවිතා කරයි. මෙම seeds වෙනස් නොකර තබා ගන්න. එසේ නොමැතිනම් ඔබට ලැබෙන පිළිතුරු බලාපොරොත්තු වන අගයට වඩා වෙනස් වනු ඇත."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1253,"status":"ok","timestamp":1696479878940,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"},"user_tz":-330},"id":"V3rMdOhVNU4i"},"outputs":[],"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","from testCases import *\n","from dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","np.random.seed(1)"]},{"cell_type":"markdown","metadata":{"id":"akMItS0BNU4k"},"source":["## 2 - Outline of the Assignment\n","\n","ඔබගේ neural network එක ගොඩනැගීම සඳහා, ඔබ \"helper functions\" කිහිපයක් implement කරනු ඇත. layers දෙකක neural network එකක් සහ layers L ගණනක් ඇති neural network එකක් ගොඩනැගීමට මෙම helper functions භාවිතා කල හැක. ඔබ විසින් implement කරන සෑම කුඩා helper functions එකක් සඳහාම implement කිරීමට අවශ්‍ය පියවර හරහා ඔබව ගෙන යන සවිස්තරාත්මක උපදෙස් ඇත. මෙම පැවරුමේ දළ සටහනක්ද පහතින් දක්වා ඇත, ඔබ විසින්:\n","\n","-  2-layer neural network එකක් සහ L-layer neural network එකක් සඳහා අවශ්‍ය w සහ b parameters initialize කරන්න.\n","- Forward propagation module එක implement කරන්න.  (පහත රූපයේ දම් පාටින් පෙන්වා ඇත).\n","      - layer එකකට අදාල Forward propagation හි linear කොටස implement කරන්න (ප්‍රතිඵලය: $Z^{[l]}$).\n","      - අපි ඔබට ACTIVATION ශ්‍රිතය (relu/sigmoid) ලබා දී ඇත.\n","      - පෙර පියවර දෙක එක කර නව [LINEAR->ACTIVATION] function එකක් නිර්මාණය කර ගන්න.\n","      - [LINEAR->RELU] ඉදිරි ශ්‍රිතය L-1 වාරයක් (layer 1 සිට layer L-1 දක්වා) භාවිතා කර අවසානයේ [LINEAR->SIGMOID] එකක් එක් කරන්න (අවසාන ස්ථරය $L$ සඳහා). මෙය ඔබට නව L_model_forward ශ්‍රිතයක් ලබා දෙයි.\n","- cost එක ගණනය කරන්න.\n","- Backward Propagation Model එක ක්‍රියාත්මක කරන්න (පහත රූපයේ රතු පැහැයෙන් දක්වා ඇත).\n","     - layer එකක backward propagation step එකක linear කොටස සම්පූර්ණ කරන්න.\n","     - අපි ඔබට activation function වල අනුක්‍රමය ලබා දී ඇත. (relu_backward/sigmoid_backward)\n","     - පෙර පියවර දෙක නව [LINEAR->activation] backword function එකකට combine කරන්න.\n","     - නව L_model_backward function [LINEAR->RELU] backward propagation L-1 වරක් යොදා [LINEAR->SIGMOID] අවසාන layer එක සඳහා යොදා ගන්න.\n","- අවසාන වශයෙන් parameters update කරන්න.\n","\n","<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n","<caption><center> **Figure 1**</center></caption><br>\n","\n","\n","**සටහන**\\\n","සෑම  forward function එකක් සඳහාම අනුරූප backward function එකක් ඇත. ඔබේ forward module එකේ සෑම පියවරකදීම ඔබ යම් අගයන් cache එකක ගබඩා කරන්නේ එබැවිනි. gradients ගණනය කිරීම සඳහා cache එකෙහි ඇති අගයන් ප්‍රයෝජනවත් වේ.  backpropagation module එක තුළ ඔබ පසුව gradients ගණනය කිරීමට cache එක භාවිතා කරනු ඇත. මෙම එක් එක් පියවරයන් නිවැරදිව සිදු කරන්නේ කෙසේද යන්න මෙම පැවරුම මඟින් ඔබට පෙන්වා ඇත."]},{"cell_type":"markdown","metadata":{"id":"hLUI8swtNU4l"},"source":["## 3 - Initialization\n","\n","ඔබ ඔබේ model එක සඳහා parameters initialize කරන helper functions දෙකක් ලියන්න. 2-layer model එකක් සඳහා parameters initialize කිරීම සඳහා පළමු function එක භාවිතා කරනු ඇත. දෙවැන්න මෙම initializing ක්‍රියාවලිය $L$-layer model වලට generalize කරයි.\n","\n","### 3.1 - 2-layer Neural Network\n","\n","**Exercise**:\\\n","2-layer neural network එකක parameters නිර්මාණය කිරීම සහ initialize කිරීම.\n","\n","**උපදෙස්**:\n","- ආකෘතියේ ව්යුහය: *LINEAR -> RELU -> LINEAR -> SIGMOID*.\n","- weight matrices (න්‍යාස) (w) සඳහා random initialization භාවිතා කරන්න. නිවැරදි හැඩය සමඟ `np.random.randn(shape)*0.01` භාවිතා කරන්න.\n","- biases (b) සඳහා zero initialization භාවිතා කරන්න. `np.zeros(shape)` භාවිත කරන්න."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"6GohkbF-NU4m","executionInfo":{"status":"ok","timestamp":1696481174306,"user_tz":-330,"elapsed":597,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"}}},"outputs":[],"source":["# GRADED FUNCTION: initialize_parameters\n","\n","def initialize_parameters(n_x, n_h, n_y):\n","    \"\"\"\n","    Argument:\n","    n_x -- size of the input layer\n","    n_h -- size of the hidden layer\n","    n_y -- size of the output layer\n","\n","    Returns:\n","    parameters -- python dictionary containing your parameters:\n","                    W1 -- weight matrix of shape (n_h, n_x)\n","                    b1 -- bias vector of shape (n_h, 1)\n","                    W2 -- weight matrix of shape (n_y, n_h)\n","                    b2 -- bias vector of shape (n_y, 1)\n","    \"\"\"\n","\n","    np.random.seed(1)\n","\n","    ### START CODE HERE ### (≈ 4 lines of code)\n","    W1 = np.random.randn(n_h, n_x)*0.01\n","    b1 = np.zeros((n_h,1))\n","    W2 = np.random.randn(n_y, n_h)*0.01\n","    b2 = np.zeros((n_y,1))\n","\n","    ### END CODE HERE ###\n","\n","    assert(W1.shape == (n_h, n_x))\n","    assert(b1.shape == (n_h, 1))\n","    assert(W2.shape == (n_y, n_h))\n","    assert(b2.shape == (n_y, 1))\n","\n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","\n","    return parameters"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":735,"status":"ok","timestamp":1696481177923,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"},"user_tz":-330},"id":"JuufyZvMNU4n","outputId":"cfd7f14d-df6e-45eb-d6e5-e4bef04ea515","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n"," [-0.01072969  0.00865408 -0.02301539]]\n","b1 = [[0.]\n"," [0.]]\n","W2 = [[ 0.01744812 -0.00761207]]\n","b2 = [[0.]]\n"]}],"source":["parameters = initialize_parameters(3,2,1)\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"id":"EgRP31EGNU4n"},"source":["**Expected output**:\n","       \n","<table style=\"width:80%\">\n","  <tr>\n","    <td> **W1** </td>\n","    <td> [[ 0.01624345 -0.00611756 -0.00528172]\n"," [-0.01072969  0.00865408 -0.02301539]] </td>\n","  </tr>\n","\n","  <tr>\n","    <td> **b1**</td>\n","    <td>[[ 0.]\n"," [ 0.]]</td>\n","  </tr>\n","  \n","  <tr>\n","    <td>**W2**</td>\n","    <td> [[ 0.01744812 -0.00761207]]</td>\n","  </tr>\n","  \n","  <tr>\n","    <td> **b2** </td>\n","    <td> [[ 0.]] </td>\n","  </tr>\n","  \n","</table>"]},{"cell_type":"markdown","metadata":{"id":"2zjh0XHsNU4o"},"source":["### 3.2 - L-layer Neural Network\n","\n","Deeper L-layer neural network එකක් initialize කිරීම වඩාත් සංකීර්ණ වන්නේ පෙර 2-layer neural network එකට වඩා තවත් බොහෝ W - weight metrices සහ b - bias vectors ඇති බැවිනි. `initialize_parameters_deep` සම්පූර්ණ කරන විට, ඔබේ එක් එක් LAYER අතර parameters වල dimansions ගැළපෙන බවට ඔබ සහතික විය යුතුය. $n^{[l]}$ යනු $l$ ස්ථරයේ ඇති node ගණන බව මතක තබා ගන්න. උදාහරණයක් ලෙස අපගේ input එක $X$ හි ප්‍රමාණය $(12288, 209)$ නම් ($m=209$ = data points  209 ක් ඇත.) එවිට:\n","\n","![Alt text](image.png)\n","\n","අපි python හි $W X + b$ ගණනය කරන විට python විසින් broadcasting සිදු කරන බව මතක තබා ගන්න. උදාහරණයක් ලෙස:\n","\n","$$ W = \\begin{bmatrix}\n","    j  & k  & l\\\\\n","    m  & n & o \\\\\n","    p  & q & r\n","\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n","    a  & b  & c\\\\\n","    d  & e & f \\\\\n","    g  & h & i\n","\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n","    s  \\\\\n","    t  \\\\\n","    u\n","\\end{bmatrix}\\tag{2}$$\n","\n","එවිට $WX + b$ යන සුළු කිරීම පහත පරිදි වනු ඇත:\n","\n","$$ WX + b = \\begin{bmatrix}\n","    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n","    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n","    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n","\\end{bmatrix}\\tag{3}  $$\n","\n","\\\n","මෙය මෙසේ සිදුවීම සඳහා,\n","\n","$$ b =\\begin{bmatrix}\n","    s  \\\\\n","    t  \\\\\n","    u\n","\\end{bmatrix}\\tag{2}$$\n","\n","යන 1 column matrix එක, පහත ඇති 3 column matrix එක බවට broadcast වීම මගින් සකස් වී ඇත.\n","\n","$$ b = \\begin{bmatrix}\n","    s  & s  & s\\\\\n","    t  & t & t \\\\\n","    u  & u & u\n","\\end{bmatrix}\\;\\;\\; $$\n"]},{"cell_type":"markdown","metadata":{"id":"iu5MpSUHNU4p"},"source":["**Exercise**: L-layer Neural Network එකක් සඳහා initialization Implement කිරීම.\n","\n","**Instructions**:\n","- model එකෙහි structure එක *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. එනම්, එහි ReLU activation function එක භාවිතා කරන layers $L-1$ක් ඇති අතර, පසුව sigmoid activation function එක භාවිතා කරන output  layer එකක් ඇත.\n","- weight matrices සඳහා random initialization භාවිතා කරන්න. මේ සඳහා `np.random.randn(shape) * 0.01` භාවිත කරන්න.\n","- biases සඳහා zeros initialization භාවිතා කරන්න. මේ සඳහා `np.zeros(shape)` භාවිත කරන්න.\n","- විවිධ ස්ථරවල ඇති nodes ගණන ($n^{[l]}$,), `layer_dims` ලෙස ඇති variable එකක ගබඩා කර ඇත. උදාහරණයක් ලෙස, පසුගිය සතියේ ලබාදුන් 2-layer neural network එක සඳහා `layer_dims` සකස් කලහොත් එය  [2,4,1] වනු ඇත: එය input layer එකෙහි features දෙකක්, hidden layer එකෙහි nodes 4ක් සහිත එක් hidden layer එකක් සහ output layer එකෙහි 1 node එකක් සහිත neural network එකකි. මෙයින් අදහස් වන්නේ එම neural network එක `W1` හි හැඩය (4,2), `b1` (4,1), `W2` (1,4) සහ `b2` (1,1) ආකාරයේ හැඩයන් ඇති neural network එකක් බවයි. දැන් ඔබ මෙය $L$ layers වලට generalize කළ යුතුය.\n","- පහත දක්වා ඇත්තේ $L=1$ (1 layer neural network එකක්) සඳහා වන implementation එකයි. එය gerenarize කර L-layer neural network එකක් සඳහා implement කරන්න.\n","\n","```python\n","    if L == 1:\n","        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n","        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n","```"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"hRlsfrE6NU4q","executionInfo":{"status":"ok","timestamp":1696482413205,"user_tz":-330,"elapsed":528,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"}}},"outputs":[],"source":["# GRADED FUNCTION: initialize_parameters_deep\n","\n","def initialize_parameters_deep(layer_dims):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the dimensions of each layer in our network\n","\n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","                    bl -- bias vector of shape (layer_dims[l], 1)\n","    \"\"\"\n","\n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layer_dims)            # number of layers in the network\n","\n","    for l in range(1, L):\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","        ### END CODE HERE ###\n","\n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","    return parameters"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":527,"status":"ok","timestamp":1696482416873,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"},"user_tz":-330},"id":"gKrxm9mENU4q","outputId":"ce312474-8d39-434a-9954-c3539567cfb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n"," [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n"," [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n"," [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n","b1 = [[0.]\n"," [0.]\n"," [0.]\n"," [0.]]\n","W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n"," [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n"," [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n","b2 = [[0.]\n"," [0.]\n"," [0.]]\n"]}],"source":["parameters = initialize_parameters_deep([5,4,3])\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"id":"iQfYWDn6NU4r"},"source":["**Expected output**:\n","       \n","<table style=\"width:80%\">\n","  <tr>\n","    <td> **W1** </td>\n","    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n"," [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n"," [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n"," [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td>\n","  </tr>\n","  \n","  <tr>\n","    <td>**b1** </td>\n","    <td>[[ 0.]\n"," [ 0.]\n"," [ 0.]\n"," [ 0.]]</td>\n","  </tr>\n","  \n","  <tr>\n","    <td>**W2** </td>\n","    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n"," [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n"," [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td>\n","  </tr>\n","  \n","  <tr>\n","    <td>**b2** </td>\n","    <td>[[ 0.]\n"," [ 0.]\n"," [ 0.]]</td>\n","  </tr>\n","  \n","</table>"]},{"cell_type":"markdown","metadata":{"id":"fyFvdI1FNU4s"},"source":["## 4 - Forward propagation module\n","\n","### 4.1 - Linear Forward\n","ඔබ දැන් parameters initialize කිරීම සිදු කර ඇත. මීලගට forward propagation module එක implement කල යුතුය. මේ සඳහා, ප්‍රථමයෙන්, අපට ඉදිරියේදීද model එක implement කිරීමේදී භාවිතා කිරීමට සිදුවන basic functions කිහිපයක් implement කරමු. මෙම functions implement කිරීම පහත අනුපිලිවෙල අනුව සිදු කරනු ලබයි.\n","\n","- LINEAR\n","- LINEAR -> ACTIVATION (මෙහිදී ACTIVATION function එක ReLU හෝ Sigmoid වේ.)\n","- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (සම්පුර්ණ model එක)\n","\n","linear forward module එක (සියලුම inputs(examples) එකවර forward propagate කිරීම සඳහා vectorize කර ඇති) පහත equation එක calculate කරනු ලබයි:\n","\n","$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n","\n","මෙහි $A^{[0]} = X$ වේ.\n","\n","**Exercise**: forward propagation එකෙහි linear part එක ගොඩ නගන්න.\n","\n","**Reminder**:\n","මෙම කොටසේ mathematical representation එක වන්නේ $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$ වේ. `np.dot()` method එක ඔබට ප්‍රයෝජනවත් වනු ඇත. නිතරම matrices වල dimensions check කර බලන්න. dimensions match නොවන අවස්ථාවලදී, `W.shape` භාවිතයෙන් W හි dimensions check කර බැලීම ඔබට උපකාරී වනු ඇත."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"rFgCfmQ-NU4t","executionInfo":{"status":"ok","timestamp":1696484571831,"user_tz":-330,"elapsed":515,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"}}},"outputs":[],"source":["# GRADED FUNCTION: linear_forward\n","\n","def linear_forward(A, W, b):\n","    \"\"\"\n","    Implement the linear part of a layer's forward propagation.\n","\n","    Arguments:\n","    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","\n","    Returns:\n","    Z -- the input of the activation function, also called pre-activation parameter\n","    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","\n","    ### START CODE HERE ### (≈ 1 line of code)\n","    Z = np.dot(W, A) + b\n","    ### END CODE HERE ###\n","\n","    assert(Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b)\n","\n","    return Z, cache"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":576,"status":"ok","timestamp":1696484576091,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"},"user_tz":-330},"id":"JeTU1idsNU4t","outputId":"e99c8972-d5c3-421f-ac77-37785f423e33"},"outputs":[{"output_type":"stream","name":"stdout","text":["Z = [[ 3.26295337 -1.23429987]]\n"]}],"source":["A, W, b = linear_forward_test_case()\n","\n","Z, linear_cache = linear_forward(A, W, b)\n","print(\"Z = \" + str(Z))"]},{"cell_type":"markdown","metadata":{"id":"cCoQdEjpNU4u"},"source":["**Expected output**:\n","\n","<table style=\"width:35%\">\n","  \n","  <tr>\n","    <td> **Z** </td>\n","    <td> [[ 3.26295337 -1.23429987]] </td>\n","  </tr>\n","  \n","</table>"]},{"cell_type":"markdown","metadata":{"id":"0s2eaVpxNU4u"},"source":["### 4.2 - Linear-Activation Forward\n","\n","මෙම notebook එකේදී, ඔබ විසින් activation functions දෙකක් භාවිත කරනු ඇත.\n","\n","- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. \\\n","  අපි ඔබට 'sigmoid'  function එක සපයා ඇත. මෙම ශ්‍රිතය outputs **දෙකක්** return කරයි:activation value \"a\" සහ \"Z\" අඩංගු \"cache\" එකකින් එම outputs දෙක සමන්විත වේ.(එය අපි අනුරූප backward function එකට ඉදිරියේදී ලබා දෙනු ලබයි). ලබා දී ඇති function එක පහත ලෙස භාවිතා කිරීමට හැකිය.\n","  \n","  ```\n","  A, activation_cache = sigmoid(Z)\n","  ```\n","\n","- **ReLU**: The mathematical formula for ReLu is $A = RELU(Z) = max(0, Z)$. \\\n","  අපි ඔබට 'ReLU'  function එක සපයා ඇත. මෙම ශ්‍රිතය outputs **දෙකක්** return කරයි:activation value \"a\" සහ \"Z\" අඩංගු \"cache\" එකකින් එම outputs දෙක සමන්විත වේ.(එය අපි අනුරූප backward function එකට ඉදිරියේදී ලබා දෙනු ලබයි). ලබා දී ඇති function එක පහත ලෙස භාවිතා කිරීමට හැකිය.\n","  ```\n","  A, activation_cache = relu(Z)\n","  ```"]},{"cell_type":"markdown","metadata":{"id":"mD4s2gBuNU4u"},"source":["පහසුව සඳහා, අපි functions දෙකක් (Linear සහ Activation) එක function එකකට (LINEAR->ACTIVATION) එකතු කර ගනු ලැබේ. එම නිසා ඔබ විසින් LINEAR forward step එක සහ ACTIVATION forward step දෙකම සිදු කරන එක function එකක් implement කරනු ඇත.\n","\n","**Exercise**\\\n","*LINEAR->ACTIVATION* layer එකෙහි  forward propagation implement කරන්න.මීට අදාල  Mathematical relation එක $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ වේ. මෙහි activation \"g\" යනු sigmoid() හෝ relu() යන දෙකෙන් එකකි. linear_forward() function එක සහ නිවැරදි activation function එක භාවිතා කිරීමට සැලකිලිමත් වෙන්න.."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"LmagFo6MNU4v","executionInfo":{"status":"ok","timestamp":1696484801867,"user_tz":-330,"elapsed":10,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"}}},"outputs":[],"source":["# GRADED FUNCTION: linear_activation_forward\n","\n","def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Implement the forward propagation for the LINEAR->ACTIVATION layer\n","\n","    Arguments:\n","    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","\n","    Returns:\n","    A -- the output of the activation function, also called the post-activation value\n","    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n","             stored for computing the backward pass efficiently\n","    \"\"\"\n","\n","    if activation == \"sigmoid\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = sigmoid(Z)\n","\n","        ### END CODE HERE ###\n","\n","    elif activation == \"relu\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","\n","        ### END CODE HERE ###\n","\n","    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1696484805463,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"},"user_tz":-330},"id":"-jNxzYY1NU4v","outputId":"37957fec-4838-4440-ce3f-58c72270dbcf"},"outputs":[{"output_type":"stream","name":"stdout","text":["With sigmoid: A = [[0.96890023 0.11013289]]\n","With ReLU: A = [[3.43896131 0.        ]]\n"]}],"source":["A_prev, W, b = linear_activation_forward_test_case()\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n","print(\"With sigmoid: A = \" + str(A))\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n","print(\"With ReLU: A = \" + str(A))"]},{"cell_type":"markdown","metadata":{"id":"w0unw-VZNU4w"},"source":["**Expected output**:\n","       \n","<table style=\"width:35%\">\n","  <tr>\n","    <td> **With sigmoid: A ** </td>\n","    <td > [[ 0.96890023  0.11013289]]</td>\n","  </tr>\n","  <tr>\n","    <td> **With ReLU: A ** </td>\n","    <td > [[ 3.43896131  0.        ]]</td>\n","  </tr>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"-jGMXXkZNU4w"},"source":["**Note**: deep learning වලදී, \"[LINEAR->ACTIVATION]\" computation එක එක් layer එකක් බව මතක තබා ගන්න. මෙම පියවර දෙක මගින් layer දෙකක් නිරුපනය නොවේ."]},{"cell_type":"markdown","metadata":{"id":"ZL7LmeO3NU4w"},"source":["### d) L-Layer Model\n","\n","$L$-layer Neural Network එක implement කිරීමේදී තවදුරටත් පහසුව සඳහා, පෙරදී implement කරන ලද function එක භාවිතා කර  (`linear_activation_forward`, RELU activation function එක සමග) $L-1$ වාරයක්, සහ (`linear_activation_forward`, SIGMOID.activation function එක සමග) අන්තිම layer එක සඳහා යොදා ගනිමින් මෙම function එක complete කරන්න.\n","\n","<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n","<caption><center> **Figure 2** : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model</center></caption><br>\n","\n","**Exercise**: ඉහත model එකෙහි forward propagation කොටස implement කරන්න..\n","\n","**Instruction**: පහත code එකෙහි, `AL` ලෙස ඇති  variable එක මගින් $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$ යන කොටස දැක්වේ. (මෙය සමහර අවස්ථාවල `Yhat`(Y hat) ලෙසද හැඳින්වේ. එනම් මෙම AL යනු $\\hat{Y}$ වේ.)\n","\n","**Tips**:\n","- ඔබ විසින් පෙරදී implement කරන ලද functions භාවිතා කරන්න.\n","- [LINEAR->RELU], (L-1) වාරයක් භාවිත කිරීමට for loop එකක් භාවිතා කරන්න.\n","- එක එක layer එකෙහි caches, \"caches\" list එකෙහි රඳවාගන්න. යම් `list` එකකට අලුත් `c` අගයක් ඇතුලත් කිරීම සඳහා ඔබට `list.append(c)` යන්න භාවිතා කල හැක."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"uBP7SN7MNU4w","executionInfo":{"status":"ok","timestamp":1696485339165,"user_tz":-330,"elapsed":530,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"}}},"outputs":[],"source":["# GRADED FUNCTION: L_model_forward\n","\n","def L_model_forward(X, parameters):\n","    \"\"\"\n","    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n","\n","    Arguments:\n","    X -- data, numpy array of shape (input size, number of examples)\n","    parameters -- output of initialize_parameters_deep()\n","\n","    Returns:\n","    AL -- last post-activation value\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n","    \"\"\"\n","\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2\n","\n","    \"\"\"\n","    number of layers in the neural network\n","    \"\"\"\n","\n","    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n","    for l in range(1, L):\n","        A_prev = A\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation='relu')\n","\n","        caches.append(cache)\n","\n","        ### END CODE HERE ###\n","\n","    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation='sigmoid')\n","\n","    caches.append(cache)\n","\n","\n","    ### END CODE HERE ###\n","\n","    assert(AL.shape == (1,X.shape[1]))\n","\n","    return AL, caches"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":504,"status":"ok","timestamp":1696485344615,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"},"user_tz":-330},"id":"vpthMcz2NU4x","outputId":"15760d0c-e0d7-4f43-88d5-0ec59db36007"},"outputs":[{"output_type":"stream","name":"stdout","text":["AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n","Length of caches list = 3\n"]}],"source":["X, parameters = L_model_forward_test_case_2hidden()\n","AL, caches = L_model_forward(X, parameters)\n","print(\"AL = \" + str(AL))\n","print(\"Length of caches list = \" + str(len(caches)))"]},{"cell_type":"markdown","metadata":{"id":"Z2wIzjI_NU4y"},"source":["<table style=\"width:50%\">\n","  <tr>\n","    <td> **AL** </td>\n","    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td>\n","  </tr>\n","  <tr>\n","    <td> **Length of caches list ** </td>\n","    <td > 3 </td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"_TPASn82NU4y"},"source":["දැන් ඔබට සම්පූර්ණ 'forward propagation' ඇති අතර එය X input එක ලබාගෙන ඔබේ predictions අඩංගු $A^{[L]}$ row vector එකක් output කරයි. එය සියලුම අතරමැදි අගයන් \"caches\" තුළ ගබඩා කර ගැනීමද සිදු කරයි. $A^{[L]}$ භාවිතයෙන්, ඔබට ඔබේ predictions වල cost එක ගණනය කළ හැක."]},{"cell_type":"markdown","metadata":{"id":"68LywO4dNU4y"},"source":["## 5 - Cost function\n","දැන් ඔබ විසින් forward සහ backward propagation implement කරනු ලබයි. මෙහිදී cost එක compute කිරීමට අවශ්‍ය වේ. මීට හේතු වන්නේ අපට model එක learn කරන බව දැනගැනීමට මෙම cost එක අවශ්‍ය වන බැවිනි.\n","\n","**Exercise**\\\n","cross-entropy cost එක $J$ ගණනය කරන්න. මේ සඳහා මෙම formula එක භාවිතා කරන්න.\n","$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"Z7xmDtcdNU4y","executionInfo":{"status":"ok","timestamp":1696485545722,"user_tz":-330,"elapsed":4,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"}}},"outputs":[],"source":["# GRADED FUNCTION: compute_cost\n","\n","def compute_cost(AL, Y):\n","    \"\"\"\n","    Implement the cost function defined by equation (7).\n","\n","    Arguments:\n","    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n","    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n","\n","    Returns:\n","    cost -- cross-entropy cost\n","    \"\"\"\n","\n","    m = Y.shape[1]\n","\n","    # Compute loss from aL and y.\n","    ### START CODE HERE ### (≈ 1 lines of code)\n","    cost = (-1/m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1-Y, np.log(1-AL)))\n","    ### END CODE HERE ###\n","\n","    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n","    assert(cost.shape == ())\n","\n","    return cost"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":540,"status":"ok","timestamp":1696485550463,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"},"user_tz":-330},"id":"Dss_0eOcNU4z","outputId":"9037f9af-d8b6-446a-dddc-f1579b13d1a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["cost = 0.2797765635793422\n"]}],"source":["Y, AL = compute_cost_test_case()\n","\n","print(\"cost = \" + str(compute_cost(AL, Y)))"]},{"cell_type":"markdown","metadata":{"id":"4duCDNi3NU4z"},"source":["**Expected Output**:\n","<table>\n","    <tr>\n","    <td>**cost** </td>\n","    <td> 0.2797765635793422</td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"1cfxWTIJNU4z"},"source":["## 6 - Backward propagation module\n","\n","forward propagation සඳහා මෙන්ම backpropagation සඳහාද ඔබ විසින් helper functions implement කල යුතුය. backpropagation භාවිතා වෙන්නේ loss function එකෙහි, parameters වලට සාපේක්ෂව gradient එක (අනුක්‍රමණය) සොයා ගැනීමට බව මතක තබා ගන්න.\n","\n","**Reminder**:\\\n","<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n","<caption><center> **Figure 3** : Forward සහ Backward propagation for *LINEAR->RELU->LINEAR->SIGMOID* <br> *දම් පාට blocks මගින් forward propagation represent කරන අතර, red blocks මගින් backward propagation represent කරනු ලබයි.*  </center></caption>\n","\n","<!--\n","For those of you who are expert in calculus (you don't need to be to do this assignment), the chain rule of calculus can be used to derive the derivative of the loss $\\mathcal{L}$ with respect to $z^{[1]}$ in a 2-layer network as follows:\n","\n","$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n","\n","In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, you use the previous chain rule and you do $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. During the backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.\n","\n","Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, you use the previous chain rule and you do $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n","\n","This is why we talk about **backpropagation**.\n","!-->\n","\n","දැන් forward propagation implement කල ආකාරයට සමාන ආකාරයකින්ම, backward propagation ද පියවර 3කින් implement කරමු.\n","- LINEAR backward\n","- LINEAR -> ACTIVATION backward. මෙහිදී ACTIVATION compute කරන්නේ ReLU හෝ sigmoid activation හි derivative (අවකලනය) එක වේ.\n","- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (සම්පුර්ණ model එක)"]},{"cell_type":"markdown","metadata":{"id":"iCO8OQOnNU4z"},"source":["### 6.1 - Linear backward\n","\n","layer $l$ සඳහා, linear part එක වන්නේ: \\\n","$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (පසුව මෙයට activation function එකක් යොදනු ලබයි.).\n","\n","ඔබ දැනටමත් $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$ යන derivative එක calculate කර ඇති සිතන්න. \\\n","ඔබට ගණනය කිරීමට අවශ්‍ය වන්නේ $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ ලෙස සිතමු. අප lectures වලදී සාකඡචා කල කරුණු අනුව, මේ 3 ම ගණනය කිරීමට  $dZ^{[l]}$ අවශ්‍ය බව අපි දනිමු. එනම්, $dZ^{[l]}$ input එකක් ලෙස දුන් විට අපට $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ යන තුනම ගණනය කර ගත හැක.\n","\n","<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n","<caption><center> **Figure 4** </center></caption>\n","\n","මෙම function එකෙහි output  වන $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ ගණනය කරනු ලබන්නේ input $dZ^{[l]}$ භාවිතයෙනි. පහත දැක්වෙන්නේ ඔබට ඒ සඳහා අවශ්‍ය වන formulas වේ.\n","$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n","$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n","$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"]},{"cell_type":"markdown","metadata":{"id":"DZB-ORK8NU40"},"source":["**Exercise**:\\\n","ඉහත සඳහන් formulas 3 භාවිතයෙන් linear_backward() function එක implement කරන්න."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"LFMxly1qNU40","executionInfo":{"status":"ok","timestamp":1696486433297,"user_tz":-330,"elapsed":554,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"}}},"outputs":[],"source":["# GRADED FUNCTION: linear_backward\n","\n","def linear_backward(dZ, cache):\n","    # Here cache is \"linear_cache\" containing (A_prev, W, b) coming from the forward propagation in the current layer\n","    \"\"\"\n","    Implement the linear portion of backward propagation for a single layer (layer l)\n","\n","    Arguments:\n","    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n","    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n","\n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","    ### START CODE HERE ### (≈ 3 lines of code)\n","    dW = np.dot(dZ, A_prev.T)/m\n","    db = np.sum(dZ, axis=1, keepdims=True) /m\n","    dA_prev = np.dot(W.T, dZ)\n","    ### END CODE HERE ###\n","\n","    assert (dA_prev.shape == A_prev.shape)\n","    assert (dW.shape == W.shape)\n","    assert (db.shape == b.shape)\n","\n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1696486437787,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"},"user_tz":-330},"id":"X4FmQzolNU40","outputId":"f4cbf8cf-a2f4-44f8-a6c8-507ba4c48edf"},"outputs":[{"output_type":"stream","name":"stdout","text":["dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n"," [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n"," [-0.4319552  -1.30987417  1.72354705  0.05070578]\n"," [-0.38981415  0.60811244 -1.25938424  1.47191593]\n"," [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n","dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n"," [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n"," [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n","db = [[-0.14713786]\n"," [-0.11313155]\n"," [-0.13209101]]\n"]}],"source":["# Set up some test inputs\n","dZ, linear_cache = linear_backward_test_case()\n","\n","dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"]},{"cell_type":"markdown","metadata":{"id":"5IV6QfKrNU40"},"source":["** Expected Output**:\n","    \n","```\n","dA_prev =\n"," [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n"," [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n"," [-0.4319552  -1.30987417  1.72354705  0.05070578]\n"," [-0.38981415  0.60811244 -1.25938424  1.47191593]\n"," [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n","dW =\n"," [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n"," [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n"," [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n","db =\n"," [[-0.14713786]\n"," [-0.11313155]\n"," [-0.13209101]]\n","```"]},{"cell_type":"markdown","metadata":{"id":"hcMfFhLENU5D"},"source":["### 6.2 - Linear-Activation backward\n","\n","මීළඟට ඔබ විසින්,  **`linear_backward`** සහ activation  එක සඳහා backward step එක යන දෙකම එකතු කර  **`linear_activation_backward`** නම් නව function එකක් implement කරනු ලබයි.\n","\n","ඔබට මෙම `linear_activation_backward` function එක implement කිරීම පහසු කිරීමට, අප විසින් activation function වල backward step එක සපයා ඇත.\n","\n","- **`sigmoid_backward`**: SIGMOID unit එකෙහි backward propagation Implementation එක මෙය වේ. පහත පරිදි ඔබය එය භාවිත කල හැකිය:\n","\n","    ```python\n","    dZ = sigmoid_backward(dA, activation_cache)\n","    ```\n","\n","- **`relu_backward`**: RELU unit එකෙහි backward propagation Implementation එක මෙය වේ. පහත පරිදි ඔබය එය භාවිත කල හැකිය:\n","\n","    ```python\n","    dZ = relu_backward(dA, activation_cache)\n","    ```\n","\\\n","\\\n","$g(.)$ යනු activation function එක නම්,\n","`sigmoid_backward` සහ `relu_backward` විසින් පහත ප්ගණනය කිරීම සිදු කරයි.\n","$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$  \n","\n","**Exercise**: \\\n","*LINEAR->ACTIVATION* layer එක සඳහා backpropagation Implement කරන්න."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"GkTYKvevNU5D","executionInfo":{"status":"ok","timestamp":1696486755513,"user_tz":-330,"elapsed":692,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"}}},"outputs":[],"source":["# GRADED FUNCTION: linear_activation_backward\n","\n","def linear_activation_backward(dA, cache, activation):\n","    \"\"\"\n","    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n","\n","    Arguments:\n","    dA -- post-activation gradient for current layer l\n","    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","\n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    linear_cache, activation_cache = cache\n","\n","    if activation == \"relu\":\n","        ### START CODE HERE ### (≈ 1 line of code)\n","        dZ = relu_backward(dA, activation_cache)\n","\n","        ### END CODE HERE ###\n","\n","    elif activation == \"sigmoid\":\n","        ### START CODE HERE ### (≈ 1 line of code)\n","        dZ = sigmoid_backward(dA, activation_cache)\n","\n","        ### END CODE HERE ###\n","\n","    ### START CODE HERE ### (≈ 1 line of code)\n","    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","    ### END CODE HERE ###\n","\n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1696486759304,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"},"user_tz":-330},"id":"49KMKFsKNU5D","outputId":"05547b62-6a9a-425c-e9ae-02510ae76eeb"},"outputs":[{"output_type":"stream","name":"stdout","text":["sigmoid:\n","dA_prev = [[ 0.11017994  0.01105339]\n"," [ 0.09466817  0.00949723]\n"," [-0.05743092 -0.00576154]]\n","dW = [[ 0.10266786  0.09778551 -0.01968084]]\n","db = [[-0.05729622]]\n","\n","relu:\n","dA_prev = [[ 0.44090989  0.        ]\n"," [ 0.37883606  0.        ]\n"," [-0.2298228   0.        ]]\n","dW = [[ 0.44513824  0.37371418 -0.10478989]]\n","db = [[-0.20837892]]\n"]}],"source":["dAL, linear_activation_cache = linear_activation_backward_test_case()\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n","print (\"sigmoid:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db) + \"\\n\")\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n","print (\"relu:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"]},{"cell_type":"markdown","metadata":{"id":"4AmqqB2oNU5E"},"source":["**Expected output with sigmoid:**\n","\n","<table style=\"width:100%\">\n","  <tr>\n","    <td>\n","        dA_prev\n","     </td>\n","     <td>\n","         [[ 0.11017994  0.01105339]\n","         [ 0.09466817  0.00949723]\n","         [-0.05743092 -0.00576154]]\n","      </td>\n","  </tr>\n","    <tr>\n","        <td>\n","            dW\n","        </td>\n","        <td>\n","            [[ 0.10266786  0.09778551 -0.01968084]]\n","        </td>\n","  </tr>\n","    <tr>\n","        <td>\n","        db\n","       </td>\n","       <td >\n","           [[-0.05729622]]\n","        </td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"RResPUThNU5E"},"source":["**Expected output with relu:**\n","\n","<table style=\"width:100%\">\n","  <tr>\n","    <td > dA_prev </td>\n","           <td > [[ 0.44090989  0.        ]\n","                  [ 0.37883606  0.        ]\n","                  [-0.2298228   0.        ]]\n","            </td>\n","\n","  </tr>\n","  \n","  <tr>\n","    <td > dW </td>\n","           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td>\n","  </tr>\n","  \n","  <tr>\n","    <td > db </td>\n","           <td > [[-0.20837892]] </td>\n","  </tr>\n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d1eVFh6VNU5F"},"source":["### 6.3 - L-Model Backward\n","\n","දැන් ඔබ මුළු network එක සඳහාම backward function එක implement කල යුතුය. පෙරදී implement කල `L_model_forward` function එකෙදි, එක් එක් iteration එකේදී, (X,W,b, and z) යන දත්ත අඩංගු cache එකක් store කරනු ලැබුවා. back propagation module එකේදී gradients ගණනය කිරීම සඳහා මෙම variables භාවිතා කරනු ලබයි. එම නිසා, `L_model_backward` function එකේදී සියලුම hidden layers හරහා iterate කල යුතුයි. මෙහිදී අප  implement කරනු ලබන්නේ back propagation බැවින් layer මත iterate කරන විට $L$ වන layer එකෙන් ආරම්භ කල යුතුය. එක් එක් layer හරහා සිදුකරන iteration වලදී එම $l$ layer එක සඳහා forward propagation වලදී store කල caches භාවිතා කිරීමට සිදු වනු ඇත. පහත Figure 5 backward pass එක සිදුවන අයුරු පෙන්වයි.\n","\n","\n","<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n","<caption><center>  **Figure 5** : Backward pass  </center></caption>\n","\n","**Initializing backpropagation** :\\\n","මෙම network එකේ output එක, $A^{[L]} = \\sigma(Z^{[L]})$ වේ.\\\n","මෙම network එක හරහා backpropagate කිරීමේදී, ඔබේ code එක,\\\n","`dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$\n","compute කිරීමට අවශ්‍ය වේ. \\\n","ඒ සඳහා, පහත formula එක භාවිතා කරන්න. (මෙම dAL ට අදාල සමීකරණය ගොඩනැගුනු ආකාරය දැනගැනීම එතරම් වැදගත් නොවේ.)\n","```\n","dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n","```\n","\n","ඉහත code එක මගින් 'dAL' ගණනය කිරීමෙන් පසු, ඔබට සාමාන්‍ය ලෙස backward propagation සිදු කල හැක. Figure 5 හිදී දුටු ආකාරයට, ඔබට දැන් 'dAL' ඔබ implement කල LINEAR->SIGMOID backward function එකට ලබා දිය හැක. මෙම function එක  L_model_forward function එක මගින් store කල caches භාවිතා කරන බව නැවත මතකයට නගා ගන්න.\n","මෙය සිදු කිරීමෙන් පසුව, ඔබ for loop එකක් භාවිතයෙන් සියලුම අනෙක් layers හරහා LINEAR->RELU backward function එක භාවිතයෙන් iterate කල යුතුය. මෙම backward propagation වලදී ඔබ විසින් සියලුම dA, dW, සහ db අගයන් 'grads' dictionary එක තුල store කල යුතුය. එසේ store කිරීම සඳහා, පහත code එක භාවිතා කරන්න:\n","\n","$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n","\n","example: $l=3$ වන layer එක සඳහා, ඉහත code එක මගින් $dW^{[l]}$, 'grads' dictionary එකේ \"dW3\" key එක යටතේ store කරනු ඇත.\n","\n","**Exercise**:\\\n","*[LINEAR->RELU] $\\times$ (L-1) -> [LINEAR -> SIGMOID]* model එක සඳහා backpropagation Implement කරන්න."]},{"cell_type":"code","execution_count":43,"metadata":{"id":"J4ldm9tKNU5F","executionInfo":{"status":"ok","timestamp":1696487670575,"user_tz":-330,"elapsed":5,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"}}},"outputs":[],"source":["# GRADED FUNCTION: L_model_backward\n","\n","def L_model_backward(AL, Y, caches):\n","    \"\"\"\n","    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n","\n","    Arguments:\n","    AL -- probability vector, output of the forward propagation (L_model_forward())\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n","                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n","\n","    Returns:\n","    grads -- A dictionary with the gradients\n","             grads[\"dA\" + str(l)] = ...\n","             grads[\"dW\" + str(l)] = ...\n","             grads[\"db\" + str(l)] = ...\n","    \"\"\"\n","    grads = {}\n","    L = len(caches) # the number of layers\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n","\n","    # Initializing the backpropagation\n","    ### START CODE HERE ### (1 line of code)\n","    dAL = dAL = - (np.divide(Y, AL) -np.divide(1-Y, 1-AL))\n","    ### END CODE HERE ###\n","\n","    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n","    ### START CODE HERE ### (approx. 2 lines)\n","\n","    current_cache = caches[-1]\n","    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(sigmoid_backward(dAL, current_cache[1]),current_cache[0])\n","\n","    ### END CODE HERE ###\n","\n","    # Loop from l=L-2 to l=0\n","    for l in reversed(range(L-1)):\n","        # lth layer: (RELU -> LINEAR) gradients.\n","        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]\n","\n","        ### START CODE HERE ### (approx. 5 lines)\n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = linear_backward(sigmoid_backward(dAL, current_cache[1]), current_cache[0])\n","        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n","        grads[\"dW\" + str(l + 1)] = dW_temp\n","        grads[\"db\" + str(l + 1)] = db_temp\n","        ### END CODE HERE ###\n","\n","    return grads"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":700,"status":"ok","timestamp":1696487674248,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"},"user_tz":-330},"id":"BTylfrSMNU5G","outputId":"6e14ef7c-720d-4eb1-dd4a-ddabd2955bdc"},"outputs":[{"output_type":"stream","name":"stdout","text":["dW1 = [[-0.38142895 -0.05436378 -0.12122851 -0.09345065]\n"," [-0.36454443 -0.04886266 -0.11465667 -0.08859687]\n"," [-0.36758766 -0.04958047 -0.11573455 -0.08940829]]\n","db1 = [[0.13978379]\n"," [0.12259085]\n"," [0.12471635]]\n","dA1 = [[ 0.01969098 -0.12970306]\n"," [ 0.09890728 -0.22545732]\n"," [ 0.1304364  -0.31335009]\n"," [ 0.03215356  0.01532388]]\n"]}],"source":["AL, Y_assess, caches = L_model_backward_test_case()\n","grads = L_model_backward(AL, Y_assess, caches)\n","print_grads(grads)"]},{"cell_type":"markdown","metadata":{"id":"pBrMfOzuNU5G"},"source":["**Expected Output**\n","\n","<table style=\"width:60%\">\n","  \n","  <tr>\n","    <td > dW1 </td>\n","           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n"," [ 0.          0.          0.          0.        ]\n"," [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td>\n","  </tr>\n","  \n","  <tr>\n","    <td > db1 </td>\n","           <td > [[-0.22007063]\n"," [ 0.        ]\n"," [-0.02835349]] </td>\n","  </tr>\n","  \n","  <tr>\n","  <td > dA1 </td>\n","           <td > [[ 0.12913162 -0.44014127]\n"," [-0.14175655  0.48317296]\n"," [ 0.01663708 -0.05670698]] </td>\n","\n","  </tr>\n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xKuVzMniNU5G"},"source":["### 6.4 - Update Parameters\n","\n","මෙම section එකේදී ඔබ විසින් gradient descent භාවිතයෙන් model එකෙහි parameters update කරනු ලබයි.\n","\n","$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n","$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n","\n","මෙහිදී $\\alpha$ යනු learning rate එකයි. updated parameters ගණනය කිරීමෙන් අනතුරුව එම updated parameters, 'parameters' dictionary එකේ store කල යුතුය."]},{"cell_type":"markdown","metadata":{"id":"dwBVzpJGNU5H"},"source":["**Exercise**: \\\n","`update_parameters()` function එක gradient descent භාවිතයෙන් Implement කරන්න.\n","\n","**Instructions**:\\\n","සියලුම $W^{[l]}$ සහ $b^{[l]}$ for $l = 1, 2, ..., L$ සඳහා gradient descent භාවිතයෙන් parameters update කරන්න."]},{"cell_type":"code","execution_count":45,"metadata":{"id":"Cvi_Jy9yNU5H","executionInfo":{"status":"ok","timestamp":1696487686672,"user_tz":-330,"elapsed":526,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"}}},"outputs":[],"source":["# GRADED FUNCTION: update_parameters\n","\n","def update_parameters(parameters, grads, learning_rate):\n","    \"\"\"\n","    Update parameters using gradient descent\n","\n","    Arguments:\n","    parameters -- python dictionary containing your parameters\n","    grads -- python dictionary containing your gradients, output of L_model_backward\n","\n","    Returns:\n","    parameters -- python dictionary containing your updated parameters\n","                  parameters[\"W\" + str(l)] = ...\n","                  parameters[\"b\" + str(l)] = ...\n","    \"\"\"\n","\n","    L = len(parameters) // 2 # number of layers in the neural network\n","\n","    # Update rule for each parameter. Use a for loop.\n","    ### START CODE HERE ### (≈ 3 lines of code)\n","    for l in range(L):\n","        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n","        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n","\n","    ### END CODE HERE ###\n","    return parameters"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1696487689885,"user":{"displayName":"Chethana Virajini","userId":"05076409931549523138"},"user_tz":-330},"id":"6zGpax41NU5H","outputId":"61016854-8f85-49a4-f397-ee83d6ad7744"},"outputs":[{"output_type":"stream","name":"stdout","text":["W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n"," [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n"," [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n","b1 = [[-0.04659241]\n"," [-1.28888275]\n"," [ 0.53405496]]\n","W2 = [[-0.55569196  0.0354055   1.32964895]]\n","b2 = [[-0.84610769]]\n"]}],"source":["parameters, grads = update_parameters_test_case()\n","parameters = update_parameters(parameters, grads, 0.1)\n","\n","print (\"W1 = \"+ str(parameters[\"W1\"]))\n","print (\"b1 = \"+ str(parameters[\"b1\"]))\n","print (\"W2 = \"+ str(parameters[\"W2\"]))\n","print (\"b2 = \"+ str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"id":"3hXvw4BwNU5H"},"source":["**Expected Output**:\n","\n","<table style=\"width:100%\">\n","    <tr>\n","    <td > W1 </td>\n","           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n"," [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n"," [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td>\n","  </tr>\n","  \n","  <tr>\n","    <td > b1 </td>\n","           <td > [[-0.04659241]\n"," [-1.28888275]\n"," [ 0.53405496]] </td>\n","  </tr>\n","  <tr>\n","    <td > W2 </td>\n","           <td > [[-0.55569196  0.0354055   1.32964895]]</td>\n","  </tr>\n","  \n","  <tr>\n","    <td > b2 </td>\n","           <td > [[-0.84610769]] </td>\n","  </tr>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"uC86b1ElNU5I"},"source":["\n","## 7 - Conclusion\n","\n","deep neural network එකක් ගොඩනැගීම සඳහා අවශ්ය සියලු functions implement කිරීම ගැන ඔබට සුබ පැතුම්!\n","\n","මෙය සමහර විට ඔබ බලාපොරොත්තු වුවාට වැඩ තරමක් දිග පැවරුමක් වන්නට ඇත, නමුත් මෙය නිම කිරීම ඔබේ අනාගතයට වැදගත් ආයෝජනයක් වනු ඇත. මෙම පැවරුමේ අවසන් වීමෙන් පසුව මෙම functions ඔබට භාවිත කල හැකි ආකාරය පෙන්වන තවත් jupyter notebook එකක් ඔබට ඉදිරියේදී ලබා දෙනු ලැබේ.\n","\n","මෙම ඊළඟ jupyter notebook එකේදී ඔබ සරල models දෙකක් තැනීමට මෙම functions භාවිතා කරනු ඇත.\n","- 2-layer neural network එකක්\n","- L-layer neural network එකක්\n","\n","cat vs non-cat images classify කිරීම සඳහා ඔබ මෙම models භාවිතා කරනු ඇත!"]}],"metadata":{"colab":{"provenance":[]},"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"c4HO0","launcher_item_id":"lSYZM"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}